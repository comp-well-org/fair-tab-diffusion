{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fairBalance import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing(X_train, target, knn, sensitive_attribute, features, drop_features, continous_features):\n",
    "    fcb = fairBalance(X_train, features, continous_features, drop_features, sensitive_attribute, target, knn = knn)\n",
    "    fcb.fit()\n",
    "    X_balanced, y_balanced = fcb.generater()\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X_train, y_train, X_test, features):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test[features])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_test, y_pred, target, sensitive_attribute, priority_group):\n",
    "    y_test = X_test[target]\n",
    "\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred[sensitive_attribute]=X_test[sensitive_attribute].tolist()\n",
    "    df_pred['truth']=y_test.tolist()\n",
    "    df_pred['pred']=y_pred\n",
    "    print(\"---- overall performance ----\")\n",
    "    performance(df_pred)\n",
    "    print(\"---- performance of different groups ----\")\n",
    "    group_comp(df_pred, sensitive_attribute, priority_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example with COMPAS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. reading the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('datasets/COMPAS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. set all required parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=5\n",
    "target='two_year_recid'\n",
    "drop_features=['race','sex']\n",
    "features=list(set(df.keys().tolist())-set(drop_features+[target]))\n",
    "continous_features=['age','juv_fel_count','juv_misd_count','juv_other_count','priors_count']\n",
    "sensitive_attribute = 'race'\n",
    "privileged_group = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. separate train-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size=0.2, stratify=df[target], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. balance the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster the original dataset into 8 clusters:\n",
      "Removing 1233 samples from the original dataset...\n"
     ]
    }
   ],
   "source": [
    "X_balanced, y_balanced = balancing(X_train, target, knn, sensitive_attribute, features, drop_features, continous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.prediction & performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Performance before balancing ======== \n",
      "---- overall performance ----\n",
      "Test Accuracy  0.6790923824959482\n",
      "Precision: 0.678359\n",
      "Recall: 0.679092\n",
      "F1: 0.675811\n",
      "---- performance of different groups ----\n",
      "+----------------+-------+-------+-------+-------+\n",
      "| Group          |    F1 |   TPR |   FPR |    PR |\n",
      "|----------------+-------+-------+-------+-------|\n",
      "| Privileged     | 0.670 | 0.443 | 0.164 | 0.272 |\n",
      "| Non-privileged | 0.676 | 0.629 | 0.277 | 0.449 |\n",
      "+----------------+-------+-------+-------+-------+\n",
      "Equal Opportunity 0.186\n",
      "Equal Odds 0.149\n",
      "Statistical Parity 0.177\n",
      "======== Performance after balancing ======== \n",
      "---- overall performance ----\n",
      "Test Accuracy  0.6418152350081038\n",
      "Precision: 0.640005\n",
      "Recall: 0.641815\n",
      "F1: 0.637885\n",
      "---- performance of different groups ----\n",
      "+----------------+-------+-------+-------+-------+\n",
      "| Group          |    F1 |   TPR |   FPR |    PR |\n",
      "|----------------+-------+-------+-------+-------|\n",
      "| Privileged     | 0.637 | 0.437 | 0.220 | 0.304 |\n",
      "| Non-privileged | 0.637 | 0.569 | 0.294 | 0.429 |\n",
      "+----------------+-------+-------+-------+-------+\n",
      "Equal Opportunity 0.133\n",
      "Equal Odds 0.103\n",
      "Statistical Parity 0.125\n"
     ]
    }
   ],
   "source": [
    "print(\"========= Performance before balancing ======== \")\n",
    "y_pred = prediction(X_train[features], X_train[target], X_test, features)\n",
    "evaluation(X_test, y_pred, target, sensitive_attribute, priority_group = 1)\n",
    "\n",
    "print(\"======== Performance after balancing ======== \")\n",
    "y_pred_bal = prediction(X_balanced, y_balanced, X_test, features)\n",
    "evaluation(X_test, y_pred_bal, target, sensitive_attribute, priority_group = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
